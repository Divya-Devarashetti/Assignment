{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8033d7a-4c05-425a-983b-b873cc28ecbe",
   "metadata": {},
   "source": [
    "# EXTRACT THE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b77f725-7568-419a-98e6-11b94fd18cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import uuid\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8077fa-1154-471a-a258-20ba8f43d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the webdriver\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c78d56-bf30-4aa2-b137-82ba75118639",
   "metadata": {},
   "source": [
    "EXTRACT ARTICLE TITLE AND ARTICLE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11604c8f-cc6e-4a07-8baa-96647c23a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the webpage\n",
    "driver.get(\"https://insights.blackcoffer.com/ai-and-ml-based-youtube-analytics-and-content-creation-tool-for-optimizing-subscriber-engagement-and-content-strategy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350ba1c0-099f-4d95-ab4a-c7e2425fcc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the page source\n",
    "page_source = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ebcc54-d2b8-4498-89c6-ff04a08eb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5aa7198-d807-488f-8ef5-f86a25dcc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the header and footer\n",
    "header = soup.find('header')\n",
    "footer = soup.find('footer')\n",
    "if header:\n",
    "    header.decompose()\n",
    "if footer:\n",
    "    footer.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a397966-20e8-4f9b-a426-2e62d7cc0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the article text\n",
    "article_text = soup.find('div', class_='article-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a353aa94-315e-4277-8820-3b6694c786cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If article text is found, get its text\n",
    "if article_text:\n",
    "    article_text = article_text.get_text()\n",
    "else:\n",
    "    article_text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f6fe0e-da16-48f4-a07f-01f8acb101fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any script or style elements\n",
    "for script in soup.find_all(['script','style']):\n",
    "    script.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68945df9-238e-4e91-8f7e-612c3adb6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any remaining non-article text\n",
    "text = '\\n'.join([line for line in article_text.split('\\n')if line.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1cc452d-c8ab-4b04-a74b-b693eb3b04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bc995d8-fa9b-45f9-b092-c950d5accc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy - Blackcoffer Insights\n"
     ]
    }
   ],
   "source": [
    "print(driver.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3630a8dc-6074-4012-862f-aa9a4f943b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy - Blackcoffer Insights\n",
      "Sign in\n",
      "Our Success Stories\n",
      "Banking Securities, and Insurance\n",
      "Energy\n",
      "Entertainment\n",
      "Fast Moving Consumer Goods\n",
      "Government & Think Tanks\n",
      "Healthcare\n",
      "Infrastructure & Real Estate\n",
      "IT\n",
      "Lifestyle & eCommerce\n",
      "Production & manufacturing\n",
      "Research & Academia\n",
      "Retail & Supply Chain\n",
      "Telecom\n",
      "What We Do\n",
      "Banking, Financials, Securities, and Insurance\n",
      "Energy\n",
      "Entertainment\n",
      "Fast Moving Consumer Goods\n",
      "Government & Think Tanks\n",
      "Healthcare\n",
      "Hospitality\n",
      "Infrastructure & Real Estate\n",
      "IT Services\n",
      "Lifestyle, eCommerce & Online Market Place\n",
      "News & Media\n",
      "Production & Manufacturing\n",
      "Research & Academia\n",
      "Retail & Supply Chain\n",
      "What We Think\n",
      "Automobiles & Components\n",
      "BFSI\n",
      "Asset and Portfolio\n",
      "Banks\n",
      "Capital Markets\n",
      "Derivatives and Securities\n",
      "Diversified Financials\n",
      "Finance & Accounting\n",
      "Insurance\n",
      "Securities and Capital Markets\n",
      "Capital Goods\n",
      "Commercial & Professional Services\n",
      "Consumer Discretionary\n",
      "Consumer Durables & Apparel\n",
      "Consumer Services\n",
      "Consumer Staples\n",
      "Food & Staples Retailing\n",
      "Food, Beverage & Tobacco\n",
      "Household & Personal Products\n",
      "Data Science\n",
      "Analytics\n",
      "Artificial Intelligence\n",
      "Big Data\n",
      "Business Analytics\n",
      "Data Visualization\n",
      "Internet of Things\n",
      "Machine Learning\n",
      "Statistics\n",
      "Energy\n",
      "DataOil\n",
      "How To\n",
      "Analytics\n",
      "Application Development\n",
      "Artificial Intelligence\n",
      "Business Analytics\n",
      "Example\n",
      "Optimization\n",
      "Projects\n",
      "Software Development\n",
      "Source Code Audit\n",
      "Statistics\n",
      "Web & Mobile App Development\n",
      "Schedule Demo\n",
      "Contact\n",
      "Sign in\n",
      "Welcome!Log into your account\n",
      "your username\n",
      "your password\n",
      "Forgot your password?\n",
      "Password recovery\n",
      "Recover your password\n",
      "your email\n",
      "Search\n",
      "Sign in\n",
      "Welcome! Log into your account\n",
      "your username\n",
      "your password\n",
      "Forgot your password? Get help\n",
      "Password recovery\n",
      "Recover your password\n",
      "your email\n",
      "A password will be e-mailed to you.\n",
      "Friday, November 8, 2024 \n",
      "Sign in / Join \n",
      "Our Success Stories\n",
      "What We Do\n",
      "What We Think\n",
      "How To\n",
      "Schedule Demo\n",
      "Contact\n",
      "FacebookLinkedinTwitterYoutube\n",
      "Our Success Stories\n",
      "AllBanking Securities, and InsuranceEnergyEntertainmentFast Moving Consumer GoodsGovernment & Think TanksHealthcareInfrastructure & Real EstateITLifestyle & eCommerceProduction & manufacturingResearch & Academia\n",
      "Our Success Stories  \n",
      "Integrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps Kubernetes\n",
      "October 24, 2024 \n",
      "Our Success Stories  \n",
      "Facial Recognition Attendance System\n",
      "October 18, 2024 \n",
      "Our Success Stories  \n",
      "Face Recognition Using DeepFace\n",
      "October 18, 2024 \n",
      "Our Success Stories  \n",
      "AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy\n",
      "August 26, 2024 \n",
      "What We Do\n",
      "AllBanking, Financials, Securities, and InsuranceEnergyEntertainmentFast Moving Consumer GoodsGovernment & Think TanksHealthcareHospitalityInfrastructure & Real EstateIT ServicesLifestyle, eCommerce & Online Market PlaceNews & Media\n",
      "What We Do  \n",
      "Face Recognition with Deepfills Framework – Deepface\n",
      "October 18, 2024 \n",
      "What We Do  \n",
      "Development of EA Robot for Automated Trading\n",
      "September 15, 2024 \n",
      "Our Success Stories  \n",
      "Enhancing Data Collection for Research Institutions: Addressing Survey Fatigue and Incorporating Verbal Communication for Richer Insights\n",
      "August 25, 2024 \n",
      "What We Do  \n",
      "AI Chatbot using LLM, Langchain, LLama\n",
      "July 7, 2024 \n",
      "What We Think\n",
      "AllAutomobiles & ComponentsBFSIAsset and PortfolioBanksCapital MarketsDerivatives and SecuritiesDiversified FinancialsFinance & AccountingInsuranceSecurities and Capital MarketsCapital Goods\n",
      "What We Think  \n",
      "Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040.\n",
      "August 24, 2023 \n",
      "What We Think  \n",
      "Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future\n",
      "August 18, 2023 \n",
      "What We Think  \n",
      "Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways\n",
      "August 18, 2023 \n",
      "What We Think  \n",
      "Rise of Cybercrime and its Effect in upcoming Future\n",
      "August 18, 2023 \n",
      "How To\n",
      "AllAnalyticsApplication DevelopmentArtificial IntelligenceBusiness AnalyticsExampleOptimizationProjectsSoftware DevelopmentSource Code AuditStatisticsWeb & Mobile App Development\n",
      "What We Do  \n",
      "AI/ML and Predictive Modeling\n",
      "February 3, 2022 \n",
      "Blackcoffer  \n",
      "Solution for Contact Centre Problems\n",
      "April 26, 2021 \n",
      "How To  \n",
      "How to Setup Custom Domain for Google App Engine Application?\n",
      "February 13, 2021 \n",
      "How To  \n",
      "Code Review Checklist\n",
      "April 10, 2020 \n",
      "Schedule Demo\n",
      "Contact\n",
      "Search  \n",
      "Home  Our Success Stories  AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber...\n",
      "Client BackgroundClient: A leading IT & tech firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaS, Marketing StrategyOrganization Size: 10+The ProblemBuilding AI and ML based YouTube analytics and content creation tool that will help youtuber to understand their subscriber’s watching behaviour, help them in content research, creation and publication. Our SolutionCreated a MERN stack web application and integrated AI models to helps youtuber to generated titles, descriptions, tags, hashtags, captions etc. Help them to check thumbnail quality, analysis on the videos using video auditor tool, analysis on comments using sentiments analysis, help to under their subscribers using churn predication AI model. Solution Architecture\n",
      "https://www.figma.com/file/WQs01mmmNBZ1SjNE2IV8Sl/Youtube-Web-App-By-SHiV?type=design&node-id=0-1&mode=design&t=Lh2jRx4bGQq6l4WU-0\n",
      "DeliverablesWeb ApplicationsSupportsMaintenance Feature EnhancementTech StackTools used\n",
      "VS code\n",
      "Language/techniques used\n",
      "React.js\n",
      "Express.js\n",
      "Node.js\n",
      "Python\n",
      "Models used\n",
      "Python libraries\n",
      "Skills used\n",
      "Data scientise\n",
      "Full Stack developer\n",
      "Databases used\n",
      "MongoDB\n",
      "Web Cloud Servers used\n",
      "Google Cloud PlatformProject Snapshots Home PageTool PageDashboardBlog PageSingle Blog PostAbout UsContact UsLogin PageTitle and Description tool PageThumbnail Quality check toolProject website urlhttps://tubetool.aiSummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy \n",
      "RELATED INSIGHTSMORE FROM AUTHOR\n",
      "Integrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps Kubernetes \n",
      "Facial Recognition Attendance System \n",
      "Face Recognition Using DeepFace \n",
      "MOST POPULAR INSIGHTS\n",
      "Advanced AI for Road Cam Threat Detection \n",
      "July 22, 2023 \n",
      "Structural Equation Modeling (SEM) \n",
      "April 12, 2019 \n",
      "Sentiment Analysis Bot for Price Prediction \n",
      "March 10, 2021 \n",
      "How Telehealth and Telemedicine helping people to fight against COVID-19 \n",
      "April 26, 2022 \n",
      " Load more RECOMMENDED INSIGHTS\n",
      "The Future of Bank Risk Management\n",
      "Audify Music Player Website in MERN Stack\n",
      "Lipsync Automation for Celebrities and Influencers\n",
      "Real-Time sentiment analysis tool – Retail Industry\n",
      "LATEST INSIGHTS\n",
      "Integrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps Kubernetes\n",
      "October 24, 2024 \n",
      "Facial Recognition Attendance System\n",
      "October 18, 2024 \n",
      "Face Recognition Using DeepFace\n",
      "October 18, 2024 \n",
      "POPULAR INSIGHTS\n",
      "Integrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps Kubernetes\n",
      "October 24, 2024 \n",
      "Facial Recognition Attendance System\n",
      "October 18, 2024 \n",
      "Face Recognition Using DeepFace\n",
      "October 18, 2024 \n",
      "POPULAR INSIGHTS CATEGORYOur Success Stories229What We Think178Blackcoffer151IT102Artificial Intelligence53Healthcare52What We Do45Big Data44\n",
      "ABOUT US\n",
      "We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-lvel human expertise.\n",
      "Contact us: hello@blackcoffer.com\n",
      "FOLLOW US\n",
      "FacebookLinkedinTwitterYoutube\n",
      "© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\n",
      "Our Success Stories\n",
      "What We Do\n",
      "What We Think\n",
      "How To\n",
      "Schedule Demo\n",
      "Contact\n"
     ]
    }
   ],
   "source": [
    "# Print the article text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e7928-d667-4f8b-955e-925f468fdb06",
   "metadata": {},
   "source": [
    "# TEXTUAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6da73c5b-b882-4a68-9e8d-e11867d8d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTML tags and special characters\n",
    "text = re.sub(r'<.*?>','',text)\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53391384-4046-4104-a1ca-5bc51fbc624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text into words \n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84ca1eb1-214f-4ddf-a945-d4d5d6133b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13858e8b-045e-4036-a135-1b415f46e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c56a22f4-14f6-44f9-aba8-fa3753d9c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the frequency of each word\n",
    "word_freq = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a70ccdd9-daf2-45d4-bfc7-45605f366e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article saved to a871b69b-9654-4579-a6c8-b209f65b3825.txt\n",
      "Top 10 most frequent words:\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 most frequent words\n",
    "print(f\"Article saved to {file_id}.txt\")\n",
    "print(\"Top 10 most frequent words:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e01bb9d9-a8b7-4d74-9843-f4de33a2e953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think: 12\n",
      "Success: 11\n",
      "Stories: 10\n",
      "October: 10\n",
      "Recognition: 9\n",
      "AI: 8\n",
      "Consumer: 8\n",
      "Analytics: 7\n",
      "Code: 7\n",
      "password: 7\n"
     ]
    }
   ],
   "source": [
    "for word, freq in word_freq.most_common(10):\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb84aa-6212-4e60-9329-a56ac4249d65",
   "metadata": {},
   "source": [
    "# COMPUTE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2832881-7c29-40d4-9da9-d1e85550d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_score(text, positive_dictionary):\n",
    "    #initialize score to 0 \n",
    "    score = 0\n",
    "\n",
    "    #Split the text into individual words\n",
    "    words = text.split()\n",
    "\n",
    "    #Iterate over each word in the text \n",
    "    for word in words :\n",
    "        #Remove punctuation and convert to lowercase \n",
    "        word = word.strip('.,!?\"\\'').lower()\n",
    "\n",
    "        #check if the word is in the positive Dictionary\n",
    "        if word in positive_dictionary:\n",
    "            #if found, increment the score by 1\n",
    "            score += 1\n",
    "\n",
    "            #Return the total score \n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60192e64-cacf-4f85-9687-a34fbc621802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "positive_dictionary = [\"happy\", \"good\", \"greet\"]\n",
    "text = \"I'm feeling happy and good today!\"\n",
    "print(calculate_positive_score(text, positive_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7f7b09e-735a-4db9-84a1-6a1643e19c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_negative_score(text, negative_dict):\n",
    "    score = 0\n",
    "    for word in text.split():\n",
    "        if word in negative_dict:\n",
    "            score += 1\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b52ac402-d4b4-44a0-8e21-1112d0a679cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    }
   ],
   "source": [
    "negative_dict = {\"bad\", \"hate\", \"sad\"}\n",
    "text = \"I hate this bad day\"\n",
    "print(calculate_negative_score(text, negative_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e83f567e-eb05-457c-9a2c-543216622f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarity_score(positive_score, negative_score):\n",
    "    \"\"\"\n",
    "    Calculate the Polarity Score using the given formula.\n",
    "\n",
    "    Args:\n",
    "        positive_score (float): The score indicating the positive sentiment.\n",
    "        negative_score (float): The score indicating the negative sentiment.\n",
    "\n",
    "    Returns:\n",
    "        float: The Polarity Score.\n",
    "    \"\"\"\n",
    "    if positive_score + negative_score == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return (positive_score - negative_score) / (positive_score + negative_score + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b710aa9f-e410-48ee-9fdf-dccbd98d61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5999994000006001\n"
     ]
    }
   ],
   "source": [
    "positive_score = 0.8\n",
    "negative_score = 0.2\n",
    "polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
    "print(polarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3b7bce4-eb00-4144-9b51-bf907e945239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
    "    \"\"\"\n",
    "    Calculate the Subjectivity Score using the given formula.\n",
    "\n",
    "    Args:\n",
    "        positive_score (float): The score indicating the positive sentiment.\n",
    "        negative_score (float): The score indicating the negative sentiment.\n",
    "        total_words (int): The total number of words after cleaning.\n",
    "\n",
    "    Returns:\n",
    "        float: The Subjectivity Score.\n",
    "    \"\"\"\n",
    "    if total_words == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return (positive_score + negative_score) / (total_words + 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4a87e13-d17d-479d-aec2-148782a6a354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0099999999\n"
     ]
    }
   ],
   "source": [
    "positive_score = 0.8\n",
    "negative_score = 0.2\n",
    "total_words = 100\n",
    "subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, total_words)\n",
    "print(subjectivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03f9ed91-f247-4e73-896b-d9555b616bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sentence_length(text):\n",
    "    \"\"\"\n",
    "    Calculate the Average Sentence Length using the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        float: The Average Sentence Length.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    sentences = text.split('.')  # Split the text into sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
    "    if len(sentences) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return len(words) / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba65f78d-7dda-4a39-b24b-a890f485eb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence. This is another sentence.\"\n",
    "average_sentence_length = calculate_average_sentence_length(text)\n",
    "print(average_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac77b785-f3ea-4fdf-b440-a41776ba7604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('cmudict')\n",
    "d = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bd9b090-bc73-4553-8285-1ffe80b09b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    try:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except KeyError:\n",
    "        return 0  # Word not found in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f713a3a4-419d-4cf6-a780-33a46e8229cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complex_words_percentage(text):\n",
    "    \"\"\"\n",
    "    Calculate the Percentage of Complex Words using the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        float: The Percentage of Complex Words.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    return (len(complex_words) / len(words)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83880c04-b215-4e16-99f3-57f33028bc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence with complex words like unrecognizable.\"\n",
    "percentage_complex_words = calculate_complex_words_percentage(text)\n",
    "print(percentage_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b6e2f73-421d-483f-b05e-36f1fd33d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sentence_length(text):\n",
    "    \"\"\"\n",
    "    Calculate the Average Sentence Length using the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        float: The Average Sentence Length.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    sentences = text.split('.')  # Split the text into sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
    "    if len(sentences) == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return len(words) / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5019302-abc8-4447-af52-4da0a5fa4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complex_words_percentage(text):\n",
    "    \"\"\"\n",
    "    Calculate the Percentage of Complex Words using the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        float: The Percentage of Complex Words.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    return (len(complex_words) / len(words)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0d61684-5c01-413e-82ce-2fd8236a5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fog_index(text):\n",
    "    \"\"\"\n",
    "    Calculate the Fog Index using the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        float: The Fog Index.\n",
    "    \"\"\"\n",
    "    average_sentence_length = calculate_average_sentence_length(text)\n",
    "    percentage_complex_words = calculate_complex_words_percentage(text)\n",
    "    return 0.4 * (average_sentence_length + (percentage_complex_words / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c5b296c-f298-4872-af2b-712f9864d34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.04\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence with complex words like unrecognizable.\"\n",
    "fog_index = calculate_fog_index(text)\n",
    "print(fog_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "286c4741-968d-4e45-b05b-7d518762ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_words_per_sentence(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "    \n",
    "    # Calculate the total number of words\n",
    "    total_words = len(text.split())\n",
    "    \n",
    "    # Calculate the total number of sentences\n",
    "    total_sentences = len(sentences)\n",
    "    \n",
    "    # Check if there are any sentences\n",
    "    if total_sentences == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate the average number of words per sentence\n",
    "    average_words_per_sentence = total_words / total_sentences\n",
    "    \n",
    "    return average_words_per_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa64c2f5-5c1a-42c7-8329-91897d863420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "text = \"This is a sample text. It has multiple sentences. Each sentence is separated by a period.\"\n",
    "average_words_per_sentence = calculate_average_words_per_sentence(text)\n",
    "print(average_words_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95856af4-b9f8-47ce-9aec-aa324d38e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_complex_words(text):\n",
    "    # Initialize the diphone dictionary\n",
    "    diphone_dict = cmudict.dict()\n",
    "    \n",
    "    # Initialize the count of complex words\n",
    "    complex_word_count = 0\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Iterate over each word\n",
    "    for word in words:\n",
    "        # Remove punctuation\n",
    "        word = ''.join(e for e in word if e.isalnum())\n",
    "        \n",
    "        # Check if the word is in the diphone dictionary\n",
    "        if word.lower() in diphone_dict:\n",
    "            # Get the phonemes for the word\n",
    "            phonemes = diphone_dict[word.lower()]\n",
    "            \n",
    "            # Check if the word has more than two syllables\n",
    "            if len(phonemes) > 2:\n",
    "                complex_word_count += 1\n",
    "    \n",
    "    return complex_word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aef2a4d7-9784-4cb8-b917-eea64153735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "text = \"This is a sample text with complex words like unrecognizable and unpredictable.\"\n",
    "complex_word_count = count_complex_words(text)\n",
    "print(complex_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c14eee90-3361-4341-82af-d8767250b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cleaned_words(text):\n",
    "    # Get the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove punctuation from the text\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and count the remaining words\n",
    "    cleaned_word_count = sum(1 for word in words if word.lower() not in stop_words)\n",
    "    \n",
    "    return cleaned_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccdf5367-8a81-4d8b-9512-84e2c2f38024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "text = \"This is a sample text with some stopwords like 'the', 'and', 'a' and punctuation like ? ! , .\"\n",
    "cleaned_word_count = count_cleaned_words(text)\n",
    "print(cleaned_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd3db46d-4cdc-4959-ad6f-a9c65258682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    # Define vowels\n",
    "    vowels = 'aeiouy'\n",
    "    \n",
    "    # Initialize syllable count\n",
    "    syllable_count = 0\n",
    "    \n",
    "    # Check if the word ends with \"es\" or \"ed\"\n",
    "    if word.endswith('es') or word.endswith('ed'):\n",
    "        return 1\n",
    "    \n",
    "    # Count the number of vowels\n",
    "    for i in range(len(word)):\n",
    "        if word[i] in vowels:\n",
    "            if i == 0:\n",
    "                syllable_count += 1\n",
    "            elif word[i-1] not in vowels and word[i-1] != 'y':\n",
    "                syllable_count += 1\n",
    "    \n",
    "    return syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57238c54-b2a5-4ed2-aa7c-4837b55604e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_syllables(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Initialize total syllable count\n",
    "    total_syllable_count = 0\n",
    "    \n",
    "    # Iterate over each word\n",
    "    for word in words:\n",
    "        # Remove punctuation\n",
    "        word = ''.join(e for e in word if e.isalnum())\n",
    "        \n",
    "        # Count the syllables in the word\n",
    "        syllable_count = count_syllables(word)\n",
    "        \n",
    "        # Add to the total syllable count\n",
    "        total_syllable_count += syllable_count\n",
    "    \n",
    "    return total_syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d741adef-bde1-4a73-9bc4-518ce2f1fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "text = \"This is a sample text with multiple syllables in each word.\"\n",
    "total_syllable_count = count_word_syllables(text)\n",
    "print(total_syllable_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50a2685e-c7b0-4059-9326-a9069e796a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    # Define the pattern for personal pronouns\n",
    "    pattern = r'\\b(I|we|my|ours|us)\\b'\n",
    "    \n",
    "    # Use regex to find the counts of personal pronouns\n",
    "    pronoun_count = len(re.findall(pattern, text, re.IGNORECASE))\n",
    "    \n",
    "    return pronoun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d335659-507c-47c3-ac0f-a261a736a37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "text = \"I am going to the US with my friends. We will have a great time.\"\n",
    "pronoun_count = count_personal_pronouns(text)\n",
    "print(pronoun_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "beb45f8e-7fbc-49ec-b84f-777f1bdaab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text into words\n",
    "words = re.findall(r'\\b\\w+\\b', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5fe7b6b3-38f7-4640-84e8-1df9056b4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the total number of characters in each word\n",
    "total_chars = sum(len(word) for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53de39a5-e61b-4ab9-bffd-d61e49de4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the average word length\n",
    "average_word_length = total_chars/ len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9ff2102-146e-4ce8-9087-1183ef130e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Length: 3.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Word Length: {average_word_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9396d-75ca-4daf-97ef-9b86e600927b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21de7c-e13a-43a6-97ff-b72ae275cb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
